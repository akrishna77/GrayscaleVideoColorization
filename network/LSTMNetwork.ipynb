{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ddlcolorize - LSTMNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijwb3fKT0lBh",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-TQpvj-p-6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import math\n",
        "\n",
        "from time import time\n",
        "\n",
        "import keras\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "\n",
        "import keras.backend as K\n",
        "\n",
        "from keras import Model\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from keras.layers import Input\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import Adam\n",
        "from skimage import color\n",
        "from multiprocessing import Pool\n",
        "from keras.layers import Conv2D, Dense, GlobalAveragePooling2D, LSTM, TimeDistributed, UpSampling2D, Dropout\n",
        "from keras.engine import Layer\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from collections import namedtuple\n",
        "from datetime import datetime\n",
        "from os import listdir\n",
        "from os.path import join, isfile\n",
        "\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gMYmBTA0oib",
        "colab_type": "text"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oPNDfgtzcE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJbFnU8c0qfF",
        "colab_type": "text"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUHxECNG0zmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59YbG0iUhUWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill $(lsof -t -i:6006)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMNu9UTz1yiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = '/content/drive/My\\ Drive/new-logs/'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tczrEEDK11Vg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3Sz6sve14Gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2w6saIM0ugW",
        "colab_type": "text"
      },
      "source": [
        "## Sampling functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k55MXA8sOJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sample:\n",
        "    def __init__(self, source_dir, dest_dir, length=3, skip=1):\n",
        "        self.source_dir = source_dir\n",
        "        self.dest_dir = dest_dir\n",
        "        self.length = length\n",
        "        self.skip = skip\n",
        "\n",
        "    def save_sliced_video(self, input_file, output_file):\n",
        "        try:\n",
        "            video = cv2.VideoCapture(input_file)\n",
        "            count = 0\n",
        "            frames = []\n",
        "            frames_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            start_frame = int(frames_count/2)\n",
        "            video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "\n",
        "            while count < self.length:\n",
        "                ret, frame = video.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                if int(video.get(cv2.CAP_PROP_POS_FRAMES)) % self.skip != 0:\n",
        "                    continue\n",
        "                frames.append(frame)\n",
        "                count += 1\n",
        "            flag = len(frames) == self.length\n",
        "            fps = video.get(cv2.CAP_PROP_FPS)\n",
        "            size = (int(video.get(cv2.CAP_PROP_FRAME_WIDTH)), int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "            out = cv2.VideoWriter(output_file, fourcc, fps, size)\n",
        "            # check if video has required length if not return false\n",
        "            # assert len(frames) == frames_per_video\n",
        "            if flag:\n",
        "                for f in frames:\n",
        "                    out.write(f)\n",
        "            video.release()\n",
        "            out.release()\n",
        "            return flag\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def slice_all(self):\n",
        "        # iterate over each file in the source directory\n",
        "        file_list = listdir(self.source_dir)\n",
        "        print(\"Start sampling %d files\" % len(file_list))\n",
        "        count = 0\n",
        "        for file_name in file_list:\n",
        "            input_file = join(self.source_dir, file_name)\n",
        "            new_file_name = \"video_\" + format(count, '05d') + \".avi\"\n",
        "            output_file = join(self.dest_dir, new_file_name)\n",
        "            if isfile(input_file):\n",
        "                flag = self.save_sliced_video(input_file, output_file)\n",
        "                if flag:\n",
        "                    count += 1\n",
        "            if count % 10 == 0:\n",
        "                print(\"Processed %d out of %d\" % (count, len(file_list)))\n",
        "        print(\"Sampling completed %d out of %d\" % (count, len(file_list)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC38dIES0xws",
        "colab_type": "text"
      },
      "source": [
        "## Video Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhQdDy-RstVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_video(file):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    file - path to video file\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    frames - frames array of the video\n",
        "    '''\n",
        "\n",
        "    video = cv2.VideoCapture(file)\n",
        "    frames = []\n",
        "    while video.isOpened():\n",
        "        ret, frame = video.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "    frames = np.asarray(frames)\n",
        "    return frames\n",
        "\n",
        "\n",
        "def get_lab_layer(frames):\n",
        "    '''\n",
        "        Parameters\n",
        "        -----------\n",
        "        frames - color/gray video frames with 3 chanels\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (rgb2lab, gray2lab) - RGB frames converted to LAB, GRAY frames converted to LAB\n",
        "    '''\n",
        "    rgb2lab_frames = []\n",
        "    gray2lab_frames = []\n",
        "\n",
        "    for frame in frames:\n",
        "        resized_frame = resize_pad_frame(frame, (default_nn_input_height, default_nn_input_width), equal_padding=True)\n",
        "\n",
        "        rgb2lab_frame = color.rgb2lab(resized_frame)\n",
        "        rgb2lab_frames.append(rgb2lab_frame)\n",
        "\n",
        "        rgb2gray_frame = color.rgb2gray(resized_frame)\n",
        "\n",
        "        #         Display Grayscale frame\n",
        "        #         cv2.imshow('grey', rgb2gray_frame)\n",
        "        #         cv2.waitKey(0)\n",
        "        #         cv2.destroyAllWindows()\n",
        "\n",
        "        gray2rgb_frame = color.gray2rgb(rgb2gray_frame)\n",
        "        lab_frame = color.rgb2lab(gray2rgb_frame)\n",
        "        gray2lab_frames.append(lab_frame)\n",
        "\n",
        "    return np.asarray(rgb2lab_frames), np.asarray(gray2lab_frames)\n",
        "\n",
        "\n",
        "def preprocess_frames(gray2lab_frames):\n",
        "    '''\n",
        "    Parameters\n",
        "    ---------\n",
        "    gray2lab_frames - LAB frames of Grayscale video\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    processed_l_layer - L Layer processed (L/50 - 1)\n",
        "    '''\n",
        "    processed = np.empty(gray2lab_frames.shape)\n",
        "\n",
        "    processed[:, :, :, 0] = np.divide(gray2lab_frames[:, :, :, 0], 50) - 1  # data loss\n",
        "    processed[:, :, :, 1] = np.divide(gray2lab_frames[:, :, :, 1], 128)\n",
        "    processed[:, :, :, 2] = np.divide(gray2lab_frames[:, :, :, 2], 128)\n",
        "\n",
        "    processed_l_layer = processed[:, :, :, np.newaxis, 0]\n",
        "\n",
        "    return processed_l_layer\n",
        "\n",
        "\n",
        "def get_resnet_records(frames):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    frames - original frames without color conversion or resizing\n",
        "\n",
        "    Details\n",
        "    -------\n",
        "    Implementation adopted from Deep Kolorization implementation\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    predictions - restnet predictions\n",
        "    '''\n",
        "    resnet_input = []\n",
        "    for frame in frames:\n",
        "        resized_frame = resize_pad_frame(frame, (resnet_input_height, resnet_input_width))\n",
        "        gray_scale_frame = cv2.cvtColor(resized_frame, cv2.COLOR_RGB2GRAY)\n",
        "        gray_scale_frame_colored = cv2.cvtColor(gray_scale_frame, cv2.COLOR_GRAY2RGB)\n",
        "        resnet_input.append(gray_scale_frame_colored)\n",
        "    resnet_input = np.asarray(resnet_input)\n",
        "\n",
        "    predictions = inception_resnet_v2_predict(resnet_input)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def getInputRange(frames_count, time_steps, current_frame):\n",
        "    '''\n",
        "    Deciding the moving window\n",
        "    '''\n",
        "    # this function should change according to our selection of\n",
        "    frame_selection = []\n",
        "    last_selection = current_frame\n",
        "    for i in range(current_frame, current_frame - time_steps, -1):\n",
        "        if (i < 0):\n",
        "            frame_selection.append(last_selection)\n",
        "        else:\n",
        "            frame_selection.append(i)\n",
        "            last_selection = i\n",
        "    frame_selection = frame_selection[::-1]\n",
        "    return frame_selection\n",
        "\n",
        "\n",
        "def get_nn_input(l_layer, resnet_out):\n",
        "    '''\n",
        "    Define the flowchroma input\n",
        "    '''\n",
        "    frames_count = l_layer.shape[0]\n",
        "    time_steps = frames_per_video\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    for i in range(frames_count):\n",
        "        frame_index_selection = getInputRange(frames_count, time_steps, i)\n",
        "        frame_selection = []\n",
        "        resnet_selection = []\n",
        "        for j in frame_index_selection:\n",
        "            frame_selection.append(l_layer[j])\n",
        "            resnet_selection.append(resnet_out[j])\n",
        "        X.append(frame_selection)\n",
        "        Y.append(resnet_selection)\n",
        "\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "    return [X, Y]\n",
        "\n",
        "\n",
        "def post_process_predictions(original_l_layers, predicted_AB_layers):\n",
        "    '''\n",
        "    Combine original L layer and predicted AB Layers\n",
        "    '''\n",
        "    time_steps = frames_per_video\n",
        "    total_frames = original_l_layers.shape[0]\n",
        "    predicted_frames = []\n",
        "    for i in range(total_frames):\n",
        "        l_layer = original_l_layers[i]\n",
        "        a_layer = np.multiply(predicted_AB_layers[i, time_steps - 1, :, :, 0],\n",
        "                              128)  # select the first frame outof three predictions\n",
        "        b_layer = np.multiply(predicted_AB_layers[i, time_steps - 1, :, :, 1], 128)\n",
        "        frame = np.empty((240, 320, 3))\n",
        "        frame[:, :, 0] = l_layer\n",
        "        frame[:, :, 1] = a_layer\n",
        "        frame[:, :, 2] = b_layer\n",
        "        # frame = color.lab2rgb(frame)\n",
        "        predicted_frames.append(frame)\n",
        "    return np.asarray(predicted_frames)\n",
        "\n",
        "\n",
        "def save_output_video(frames, output_file):\n",
        "    '''\n",
        "    Save the output video\n",
        "    '''\n",
        "    fps = 20\n",
        "    size = (320, 240)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(output_file, fourcc, fps, size)\n",
        "    for frame in frames:\n",
        "        final_out = color.lab2rgb(frame)\n",
        "        final_out_write_video = final_out * 255  # color.lab2rgb results values in [0,1]\n",
        "        final_out_write_video = final_out_write_video.astype(np.uint8)\n",
        "        out.write(final_out_write_video)\n",
        "    out.release()\n",
        "\n",
        "\n",
        "def process_test_file(file):\n",
        "    # Pre-processing\n",
        "    frames = get_video(dir_test+'/'+file)\n",
        "    (rgb2lab_frames, gray2lab_frames) = get_lab_layer(frames)\n",
        "    processed_l_layer = preprocess_frames(gray2lab_frames)\n",
        "    print('running resnet model')\n",
        "    predictions = get_resnet_records(frames)\n",
        "    print('Combining L layer and resnet out')\n",
        "    X = get_nn_input(processed_l_layer, predictions)\n",
        "\n",
        "    # Predicting\n",
        "    ckpts = glob.glob(\"/content/drive/My Drive/new-checkpoints-3/*.hdf5\")\n",
        "    latest_ckpt = max(ckpts, key=os.path.getctime)\n",
        "    print(\"loading from checkpoint:\", latest_ckpt)\n",
        "    model = load_model(latest_ckpt, custom_objects={'FusionLayer': FusionLayer})\n",
        "    predictions = []\n",
        "    for i in range(X[0].shape[0]):\n",
        "        predictions.append(model.predict([X[0][i:i + 1], X[1][i:i + 1]])[0])  # shape is (1, 3, 240, 320, 2)\n",
        "    predictions = np.asarray(predictions)\n",
        "    print(\"Flowchroma model predictions calculated\")\n",
        "\n",
        "    # Post processing\n",
        "    frame_predictions = post_process_predictions(gray2lab_frames[:, :, :, 0], predictions)\n",
        "\n",
        "    save_output_video(frame_predictions, dir_test_results+ '/' + file.split('.')[0] + '.avi')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yln5Ozcdut3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize_pad_frame(img, size, pad_color=255, equal_padding=True):\n",
        "    \"\"\"\n",
        "    Resize the frame,\n",
        "    If image is a horizontal one first match the horizontal axis then resize vertical axis and fill the remaining\n",
        "    with padding color, similar process for vertical images\n",
        "    :param equal_padding:\n",
        "    :param img: frame to be resized\n",
        "    :param size: final frame size\n",
        "    :param pad_color: color of tha padding\n",
        "    :return: re-sized frame\n",
        "    \"\"\"\n",
        "    h, w = float(img.shape[0]), float(img.shape[1])\n",
        "    expected_height, expected_width = size\n",
        "\n",
        "    # interpolation method\n",
        "    if h > expected_height or w > expected_width:  # shrinking image\n",
        "        interp = cv2.INTER_AREA\n",
        "    else:  # stretching image\n",
        "        interp = cv2.INTER_CUBIC\n",
        "\n",
        "    # aspect ratio of image\n",
        "    aspect = w / h\n",
        "\n",
        "    # compute scaling and pad sizing\n",
        "    if aspect >= 1:  # horizontal image\n",
        "        new_w = expected_width\n",
        "        new_h = np.round(new_w / aspect).astype(int)\n",
        "        if expected_height >= new_h:\n",
        "            if equal_padding:\n",
        "                pad_vert = (expected_height - new_h) / 2.0\n",
        "                pad_top, pad_bot = np.floor(pad_vert).astype(int), np.ceil(pad_vert).astype(int)\n",
        "                pad_left, pad_right = 0, 0\n",
        "            else:\n",
        "                pad_vert = (expected_height - new_h)\n",
        "                pad_top, pad_bot = 0, pad_vert\n",
        "                pad_left, pad_right = 0, 0\n",
        "        else:\n",
        "            new_h = expected_height\n",
        "            new_w = np.round(new_h * aspect).astype(int)\n",
        "            if equal_padding:\n",
        "                pad_horz = (expected_width - new_w) / 2\n",
        "                pad_left, pad_right = np.floor(pad_horz).astype(int), np.ceil(pad_horz).astype(int)\n",
        "                pad_top, pad_bot = 0, 0\n",
        "            else:\n",
        "                pad_horz = (expected_width - new_w)\n",
        "                pad_left, pad_right = 0, pad_horz\n",
        "                pad_top, pad_bot = 0, 0\n",
        "\n",
        "    elif aspect < 1:  # vertical image\n",
        "        new_h = expected_height\n",
        "        new_w = np.round(new_h * aspect).astype(int)\n",
        "        if expected_width >= new_w:\n",
        "            if equal_padding:\n",
        "                pad_horz = (expected_width - new_w) / 2\n",
        "                pad_left, pad_right = np.floor(pad_horz).astype(int), np.ceil(pad_horz).astype(int)\n",
        "                pad_top, pad_bot = 0, 0\n",
        "            else:\n",
        "                pad_horz = (expected_width - new_w)\n",
        "                pad_left, pad_right = 0, pad_horz\n",
        "                pad_top, pad_bot = 0, 0\n",
        "        else:\n",
        "            new_w = expected_width\n",
        "            new_h = np.round(new_w / aspect).astype(int)\n",
        "            if equal_padding:\n",
        "                pad_vert = (expected_height - new_h) / 2.0\n",
        "                pad_top, pad_bot = np.floor(pad_vert).astype(int), np.ceil(pad_vert).astype(int)\n",
        "                pad_left, pad_right = 0, 0\n",
        "            else:\n",
        "                pad_vert = (expected_height - new_h)\n",
        "                pad_top, pad_bot = 0, pad_vert\n",
        "                pad_left, pad_right = 0, 0\n",
        "\n",
        "    # set pad color\n",
        "    if len(img.shape) is 3 and not isinstance(pad_color,\n",
        "                                              (list, tuple, np.ndarray)):  # color image but only one color provided\n",
        "        pad_color = [pad_color] * 3\n",
        "\n",
        "    # scale and pad\n",
        "    scaled_img = cv2.resize(img, (new_w, new_h), interpolation=interp)\n",
        "    scaled_img = cv2.copyMakeBorder(scaled_img, pad_top, pad_bot, pad_left, pad_right,\n",
        "                                    borderType=cv2.BORDER_CONSTANT, value=pad_color)\n",
        "\n",
        "    return scaled_img\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsW9LSfxuwpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FrameExtractor:\n",
        "    def __init__(self, input_dir, output_dir, type):\n",
        "        self.input_dir = input_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.type = type\n",
        "\n",
        "    @staticmethod\n",
        "    def write_frame_to_image(self, file):\n",
        "        \"\"\"\n",
        "        Read content from AVI file and save frames as images\n",
        "            :param file: video file\n",
        "            :param output_file: output image file\n",
        "        \"\"\"\n",
        "        video = cv2.VideoCapture(file)\n",
        "        frames = []\n",
        "        while video.isOpened():\n",
        "            ret, frame = video.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frames.append(frame)\n",
        "\n",
        "        for i, frame in enumerate(frames):\n",
        "            frame_name = join(self.output_dir, file.split(\".\")[0].split(\"/\")[-1] + \"_img_{:05d}\" + '.' + self.type).format(i)\n",
        "            cv2.imwrite(frame_name, frame)\n",
        "\n",
        "\n",
        "\n",
        "    def write_all(self):\n",
        "        makedirs(self.output_dir, exist_ok=True)\n",
        "        \n",
        "        file_list = []\n",
        "        for file_name in listdir(self.input_dir):\n",
        "            if file_name.endswith(\".avi\"):\n",
        "                file_list.append(join(self.input_dir, file_name))\n",
        "        file_list = sorted(file_list)\n",
        "\n",
        "        print(\"Start processing %d files\" % len(file_list))\n",
        "        for i in range(len(file_list)):\n",
        "            self.write_frame_to_image(self, file_list[i])\n",
        "            if i % 10 == 0:\n",
        "                print(\"Processed %d out of %d\" % (i, len(file_list)))\n",
        "        print(\"Saving images completed.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NfAuu7D01aY",
        "colab_type": "text"
      },
      "source": [
        "## Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR_RsgqbtA2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"\n",
        "    Generates data for the Keras model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 resnet_path,\n",
        "                 lab_path,\n",
        "                 file_ids,\n",
        "                 batch_size=2,\n",
        "                 time_steps=3,\n",
        "                 h=240,\n",
        "                 w=320,\n",
        "                 shuffle=True):\n",
        "\n",
        "        self.resnet_path = resnet_path\n",
        "        self.lab_path = lab_path\n",
        "        self.file_ids = file_ids\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.time_steps = time_steps\n",
        "        self.h = h\n",
        "        self.w = w\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Denotes the number of batches per epoch\n",
        "        \"\"\"\n",
        "        return int(np.floor(len(self.file_ids) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Generate one batch of data\n",
        "        \"\"\"\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        batch_file_ids = [self.file_ids[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        return self.__data_generation(batch_file_ids)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Updates indexes after each epoch\n",
        "        \"\"\"\n",
        "        self.indexes = np.arange(len(self.file_ids))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, batch_file_ids):\n",
        "        \"\"\"\n",
        "        Generates data containing batch_size samples\n",
        "        \"\"\"\n",
        "\n",
        "        x = [np.empty((self.batch_size, self.time_steps, self.h, self.w, 1)),\n",
        "             np.empty((self.batch_size, self.time_steps, 1000))]\n",
        "\n",
        "        y = np.empty((self.batch_size, self.time_steps, self.h, self.w, 2))\n",
        "\n",
        "        # Generate data\n",
        "        for i, file_id in enumerate(batch_file_ids):\n",
        "            lab_record = np.load('{0}/lab_record_{1}.npy'.format(self.lab_path, file_id))\n",
        "            resnet_record = np.load('{0}/resnet_record_{1}.npy'.format(self.resnet_path, file_id))\n",
        "\n",
        "            x[0][i, :, :, :, 0] = lab_record[:, :, :, 0]\n",
        "            x[1][i, :, :] = resnet_record\n",
        "\n",
        "            y[i, :, :, :, :] = lab_record[:, :, :, 1:]\n",
        "\n",
        "        return x, y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE-eYvwatDJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageRecord:\n",
        "    def __init__(self, input_dir, output_dir, size, equal_padding):\n",
        "        self.input_dir = input_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.size = size\n",
        "        self.equal_padding = equal_padding\n",
        "\n",
        "    @staticmethod\n",
        "    def write_to_csv(file, output_file, size, equal_padding):\n",
        "        \"\"\"\n",
        "        Read content from AVI file and resize frames and convert frames to LAB color space\n",
        "        :param file: video file\n",
        "        :param output_file: LAB output file\n",
        "        :param size: (width, height)\n",
        "        :param equal_padding: True/False\n",
        "        \"\"\"\n",
        "        video = cv2.VideoCapture(file)\n",
        "        frames = []\n",
        "        while video.isOpened():\n",
        "            ret, frame = video.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = resize_pad_frame(frame, size, equal_padding=equal_padding)\n",
        "            # frame = cv2.cvtColor(frame, cv2.COLOR_RGB2LAB)\n",
        "            frame = color.rgb2lab(frame)\n",
        "            frames.append(frame)\n",
        "        frames = np.asarray(frames)\n",
        "\n",
        "        # LAB layers should be brought to [-1, +1] region\n",
        "        frames[:, :, :, 0] = np.divide(frames[:, :, :, 0], 50) - 1\n",
        "        frames[:, :, :, 1] = np.divide(frames[:, :, :, 1], 128)\n",
        "        frames[:, :, :, 2] = np.divide(frames[:, :, :, 2], 128)\n",
        "\n",
        "        np.save(output_file, frames)\n",
        "\n",
        "    def write_all(self):\n",
        "        file_list = []\n",
        "        for file_name in listdir(self.input_dir):\n",
        "            file_list.append(join(self.input_dir, file_name))\n",
        "        file_list = sorted(file_list)\n",
        "\n",
        "        print(\"Start processing %d files\" % len(file_list))\n",
        "        for i in range(len(file_list)):\n",
        "            self.write_to_csv(file_list[i], join(self.output_dir, \"lab_record_\" + format(i, '05d')), self.size,\n",
        "                              self.equal_padding)\n",
        "            if i % 10 == 0:\n",
        "                print(\"Processed %d out of %d\" % (i, len(file_list)))\n",
        "        print(\"LAB conversion completed\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZfH9MTxtchw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResnetRecordCreator:\n",
        "    def __init__(self, video_dir, image_dir, record_dir):\n",
        "        self.video_dir = video_dir\n",
        "        self.image_dir = image_dir\n",
        "        self.record_dir = record_dir\n",
        "\n",
        "    def convert_all(self, file_list):\n",
        "        \"\"\"\n",
        "        Convert videos in the source directory to images\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        video_frames = []\n",
        "        video_indexes = []\n",
        "        for file_name in file_list:\n",
        "            input_file = join(self.video_dir, file_name)\n",
        "\n",
        "            # self.convert_video_to_images(input_file, sample_count)\n",
        "            sample_index = int(re.search(\"video_(.*).avi\", file_name).group(1))\n",
        "            resized_frames = self.convert_video_to_images(input_file)\n",
        "\n",
        "            assert len(resized_frames) == frames_per_video\n",
        "\n",
        "            video_indexes.append(sample_index)\n",
        "            video_frames += resized_frames\n",
        "            print(\"Video %d converted to frames\" % sample_index)\n",
        "        return [video_indexes, np.asarray(video_frames)]\n",
        "\n",
        "    def convert_video_to_images(self, input_file):\n",
        "        \"\"\"\n",
        "        convert a single video file to images\n",
        "        :param input_file:\n",
        "        :param sample_count:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        video = cv2.VideoCapture(input_file)\n",
        "        frames = []\n",
        "        while video.isOpened():\n",
        "            ret, frame = video.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = resize_pad_frame(frame, (resnet_input_height, resnet_input_width))\n",
        "            gray_scale_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "            gray_scale_frame_colored = cv2.cvtColor(gray_scale_frame, cv2.COLOR_GRAY2RGB)\n",
        "            frames.append(gray_scale_frame_colored)\n",
        "        return frames\n",
        "\n",
        "    def predict_all(self, video_details):\n",
        "        video_indexes, video_frames = video_details\n",
        "        predictions = inception_resnet_v2_predict(video_frames)\n",
        "        self.write_files(video_indexes, predictions)\n",
        "\n",
        "    def write_files(self, video_indexes, predictions):\n",
        "        for i in range(len(video_indexes)):\n",
        "            frames_start = i * frames_per_video\n",
        "            frames_end = frames_start + frames_per_video\n",
        "            file_index = video_indexes[i]\n",
        "            file_content = predictions[frames_start:frames_end]\n",
        "            output_file = join(self.record_dir, \"resnet_record_\" + format(file_index, \"05d\"))\n",
        "            np.save(output_file, file_content)\n",
        "\n",
        "    def process_all(self):\n",
        "        file_list = listdir(self.video_dir)\n",
        "        file_list = sorted(file_list)\n",
        "\n",
        "        for chunk in self.chunks(file_list, resnet_video_chunk_size):\n",
        "            # convert to images of the size 299x299\n",
        "            video_details = self.convert_all(chunk)\n",
        "            # pass those images to RestNet\n",
        "            self.predict_all(video_details)\n",
        "\n",
        "            print(\"Chunk completed\")\n",
        "\n",
        "    @staticmethod\n",
        "    def chunks(l, n):\n",
        "        \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
        "        for i in range(0, len(l), n):\n",
        "            yield l[i:i + n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh75Ag9Utxfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_file(index):\n",
        "    lab_record = np.load(join(dir_lab_records, \"lab_record_\" + format(index, '05d') + \".npy\"))\n",
        "    resnet_record = np.load(join(dir_resnet_csv, \"resnet_record_\" + format(index, \"05d\") + \".npy\"))\n",
        "    for i in range(frames_per_video):\n",
        "        np.save(join(dir_frame_lab_records, \"lab_record_\" + format(index * frames_per_video + i, '08d')), lab_record[i])\n",
        "        np.save(join(dir_frame_resnet_records,\"resnet_record_\" + format(index * frames_per_video + i, '08d')), resnet_record[i])\n",
        "    print('Index ' + str(index) + ' done')\n",
        "\n",
        "def prepare_image_data():\n",
        "    lab_records = listdir(dir_lab_records)\n",
        "    resnet_records = listdir(dir_resnet_csv)\n",
        "\n",
        "    assert len(lab_records) == len(resnet_records)\n",
        "    n = len(lab_records)\n",
        "\n",
        "    file_indexes = [x for x in range(n)]\n",
        "    pool = Pool(2)\n",
        "    pool.map(process_file, file_indexes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ledtdo07w0",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p_LIJYRvrr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FusionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(FusionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        enc_out, incep_out, rnn_out = inputs\n",
        "        enc_out_shape = enc_out.shape.as_list()\n",
        "        batch_size, time_steps, h, w, _ = map(lambda x: -1 if x is None else x, enc_out_shape)\n",
        "\n",
        "        def _repeat(emb):\n",
        "            # keep batch_size, time_steps axes unchanged\n",
        "            # while replicating features h*w times\n",
        "            emb_rep = K.tile(emb, [1, 1, h * w])\n",
        "            return K.reshape(emb_rep, (batch_size, time_steps, h, w, emb.shape[2]))\n",
        "\n",
        "        incep_rep = _repeat(incep_out)\n",
        "        rnn_rep = _repeat(rnn_out)\n",
        "        return K.concatenate([enc_out, incep_rep, rnn_rep], axis=4)\n",
        "\n",
        "    def compute_output_shape(self, input_shapes):\n",
        "        enc_out_shape, incep_out_shape, rnn_out_shape = input_shapes\n",
        "\n",
        "        # Must have 3 tensors as input\n",
        "        assert input_shapes and len(input_shapes) == 3\n",
        "\n",
        "        # Batch size of the three tensors must match\n",
        "        assert enc_out_shape[0] == incep_out_shape[0] == rnn_out_shape[0]\n",
        "\n",
        "        # Number of time steps of the three tensors must match\n",
        "        assert enc_out_shape[1] == incep_out_shape[1] == rnn_out_shape[1]\n",
        "\n",
        "        # batch_size, time_steps, h, w, enc_out_depth = map(lambda x: -1 if x == None else x, enc_out_shape)\n",
        "        batch_size, time_steps, h, w, enc_out_depth = enc_out_shape\n",
        "        final_depth = enc_out_depth + incep_out_shape[2] + rnn_out_shape[2]\n",
        "        return batch_size, time_steps, h, w, final_depth\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X6wEdlrvskT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FlowChroma:\n",
        "    def __init__(self, inputs):\n",
        "        # provide encoder input and inception ResNet's output as model inputs\n",
        "        self.enc_input, self.incep_out = inputs\n",
        "\n",
        "    @staticmethod\n",
        "    def _encoder(encoder_input):\n",
        "        x = TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2), name='encoder_conv1')(\n",
        "            encoder_input)\n",
        "        x = TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same'), name='encoder_conv2')(x)\n",
        "        x = TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2), name='encoder_conv3')(x)\n",
        "        x = TimeDistributed(Conv2D(256, (3, 3), activation='relu', padding='same'), name='encoder_conv4')(x)\n",
        "        x = TimeDistributed(Conv2D(256, (3, 3), activation='relu', padding='same', strides=2), name='encoder_conv5')(x)\n",
        "        x = TimeDistributed(Conv2D(512, (3, 3), activation='relu', padding='same'), name='encoder_conv6')(x)\n",
        "        x = TimeDistributed(Conv2D(512, (3, 3), activation='relu', padding='same'), name='encoder_conv7')(x)\n",
        "        x = TimeDistributed(Conv2D(256, (3, 3), activation='relu', padding='same'), name='encoder_conv8')(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _rnn(rnn_input):\n",
        "        x = LSTM(256, return_sequences=True, name='rnn_lstm1')(rnn_input)\n",
        "        x = LSTM(256, return_sequences=True, name='rnn_lstm2')(x)\n",
        "        x = TimeDistributed(Dense(256, activation='relu'), name='rnn_dense1')(x)\n",
        "        x = TimeDistributed(Dropout(0.2), name='rnn_dropout')(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _decoder(decoder_input):\n",
        "        x = TimeDistributed(Conv2D(256, (1, 1), activation='relu'))(decoder_input)\n",
        "        x = TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same'), name='decoder_conv1')(x)\n",
        "        x = TimeDistributed(UpSampling2D((2, 2)), name='decoder_upsamp1')(x)\n",
        "        x = TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same'), name='decoder_conv2')(x)\n",
        "        x = TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same'), name='decoder_conv3')(x)\n",
        "        x = TimeDistributed(UpSampling2D((2, 2)), name='decoder_upsamp2')(x)\n",
        "        x = TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), name='decoder_conv4')(x)\n",
        "        x = TimeDistributed(Conv2D(2, (3, 3), activation='tanh', padding='same'), name='decoder_conv5')(x)\n",
        "        x = TimeDistributed(UpSampling2D((2, 2)), name='decoder_upsamp3')(x)\n",
        "        return x\n",
        "\n",
        "    def build(self):\n",
        "        x = self._encoder(self.enc_input)\n",
        "        x = TimeDistributed(GlobalAveragePooling2D())(x)\n",
        "        x = self._rnn(x)\n",
        "        x = Model(inputs=self.enc_input, outputs=x)\n",
        "\n",
        "        rnn_out = x.get_layer(name='rnn_dropout').output\n",
        "        enc_out = x.get_layer(name='encoder_conv8').output\n",
        "\n",
        "        x = FusionLayer()([enc_out, self.incep_out, rnn_out])\n",
        "        x = self._decoder(x)\n",
        "\n",
        "        model = Model(inputs=[self.enc_input, self.incep_out], outputs=x)\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_model_summaries(model):\n",
        "        model.summary(line_length=150)\n",
        "        plot_model(model, to_file='flowchroma.png')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYGvDbya1HsA",
        "colab_type": "text"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr6JEAq3q1aG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir_root = '/content/drive/My Drive'\n",
        "dir_originals = join(dir_root, 'original')\n",
        "dir_sampled = join(dir_root, 'sampled')\n",
        "dir_resnet_images = join(dir_root, 'resized_resnet_images')\n",
        "dir_resnet_csv = join(dir_root, 'resnet_csv_records')\n",
        "dir_lab_records = join(dir_root, 'lab_records')\n",
        "dir_tfrecord = join(dir_root, 'tfrecords')\n",
        "dir_test = join(dir_root, 'test')\n",
        "dir_test_results = join(dir_root, 'test_results')\n",
        "dir_frame_lab_records = join(dir_root, 'frame_lab_records')\n",
        "dir_frame_resnet_records = join(dir_root, 'frame_resnet_records')\n",
        "checkpoint_url = join(dir_root,\"updated_resnet_v2.h5\")\n",
        "\n",
        "frames_per_video = 3\n",
        "default_nn_input_height = 240\n",
        "default_nn_input_width = 320\n",
        "resnet_input_height = 299\n",
        "resnet_input_width = 299\n",
        "resnet_video_chunk_size = 100\n",
        "resnet_batch_size = 100\n",
        "\n",
        "training_set_size = 2000\n",
        "test_set_size = 239\n",
        "validation_set_size = 200\n",
        "\n",
        "resnet_output = 1000\n",
        "\n",
        "\n",
        "def progressive_filename_generator(pattern='file_{}.ext'):\n",
        "    for i in itertools.count():\n",
        "        yield pattern.format(i)\n",
        "\n",
        "# !mkdir $dir_originals\n",
        "# !mkdir $dir_sampled\n",
        "# !mkdir $dir_resnet_images\n",
        "# !mkdir $dir_resnet_csv\n",
        "# !mkdir $dir_lab_records\n",
        "# !mkdir $dir_tfrecord\n",
        "# !mkdir $dir_test\n",
        "# !mkdir $dir_test_results\n",
        "# !mkdir $dir_frame_lab_records\n",
        "# !mkdir $dir_frame_resnet_records"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgvOux58u0oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def frame_extract(source, output):\n",
        "  frameExtractor = FrameExtractor(source, output, 'jpeg')\n",
        "  frameExtractor.write_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn5TGo-X1LKB",
        "colab_type": "text"
      },
      "source": [
        "## Inception Feature Extractor Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORXkW5tiub_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "x = InceptionResNetV2(include_top=True, weights='imagenet')\n",
        "# load_model(checkpoint_url)\n",
        "y = x.output\n",
        "model = Model(inputs=x.input, outputs=y)\n",
        "\n",
        "\n",
        "def inception_resnet_v2_predict(images):\n",
        "    images = images.astype(np.float32)\n",
        "    predictions = model.predict(preprocess_input(images))\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PymyaFI1Pz4",
        "colab_type": "text"
      },
      "source": [
        "## Calling Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27R0d1UntJFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lab_image_record():\n",
        "  imageRecord = ImageRecord(dir_sampled, dir_lab_records, (default_nn_input_height, default_nn_input_width), True)\n",
        "  imageRecord.write_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4BWjb76s-FB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference():\n",
        "  files = listdir(dir_test)\n",
        "  for file in files:\n",
        "      process_test_file(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OVYIb3FsZnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sampling():\n",
        "  sample = Sample(dir_originals, dir_sampled, frames_per_video, 3)\n",
        "  sample.slice_all()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7bB-DbdtkBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_record():\n",
        "    resnetRecordConverter = ResnetRecordCreator(dir_sampled, dir_resnet_images, dir_resnet_csv)\n",
        "    resnetRecordConverter.process_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTPWvxLv1R4w",
        "colab_type": "text"
      },
      "source": [
        "## Model Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRvcNmWRqS8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "def train():\n",
        "  resnet_path = dir_resnet_csv\n",
        "  lab_path = dir_lab_records\n",
        "  val_split_ratio = 0.3\n",
        "  lr = 0.0001\n",
        "  train_batch_size = 4\n",
        "  val_batch_size = 4\n",
        "  n_epochs_to_train = 250\n",
        "  ckpt_period = 3\n",
        "\n",
        "  time_steps, h, w = frames_per_video, default_nn_input_height, default_nn_input_width\n",
        "\n",
        "  initial_epoch = 0\n",
        "  ckpts = glob.glob(\"/content/drive/My Drive/new-checkpoints-3/*.hdf5\")\n",
        "  if len(ckpts) != 0:\n",
        "      # there are ckpts\n",
        "      latest_ckpt = max(ckpts, key=os.path.getctime)\n",
        "      print(\"loading from checkpoint:\", latest_ckpt)\n",
        "      initial_epoch = int(latest_ckpt[latest_ckpt.find(\"-epoch-\") + len(\"-epoch-\"):latest_ckpt.rfind(\"-lr-\")])\n",
        "      model = load_model(latest_ckpt, custom_objects={'FusionLayer': FusionLayer})\n",
        "\n",
        "  else:\n",
        "      # no ckpts\n",
        "      enc_input = Input(shape=(time_steps, h, w, 1), name='encoder_input')\n",
        "      incep_out = Input(shape=(time_steps, 1000), name='inception_input')\n",
        "\n",
        "      model = FlowChroma([enc_input, incep_out]).build()\n",
        "\n",
        "  opt = Adam(lr=lr)\n",
        "  model.compile(optimizer=opt, loss='mse', metrics=['accuracy'])\n",
        "  # generate_model_summaries(model)\n",
        "\n",
        "  n_lab_records = len(glob.glob('{0}/*.npy'.format(dir_lab_records)))\n",
        "  n_resnet_records = len(glob.glob('{0}/*.npy'.format(dir_resnet_csv)))\n",
        "\n",
        "  assert n_lab_records == n_resnet_records\n",
        "\n",
        "  val_split = int(math.floor(n_lab_records * val_split_ratio))\n",
        "\n",
        "  dataset = {\n",
        "      \"validation\": ['{0:05}'.format(i) for i in range(val_split)],\n",
        "      \"train\": ['{0:05}'.format(i) for i in range(val_split, n_lab_records)]\n",
        "  }\n",
        "\n",
        "  basic_generator_params = {\n",
        "      \"resnet_path\": resnet_path,\n",
        "      \"lab_path\": lab_path,\n",
        "      \"time_steps\": time_steps,\n",
        "      \"h\": h,\n",
        "      \"w\": w\n",
        "  }\n",
        "  # generators\n",
        "  training_generator = DataGenerator(**basic_generator_params,\n",
        "                                    file_ids=dataset['train'],\n",
        "                                    batch_size=train_batch_size)\n",
        "\n",
        "  validation_generator = DataGenerator(**basic_generator_params,\n",
        "                                      file_ids=dataset['validation'],\n",
        "                                      batch_size=val_batch_size)\n",
        "\n",
        "  os.makedirs(\"/content/drive/My Drive/new-checkpoints-3\", exist_ok=True)\n",
        "  file_path = \"/content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-{epoch:05d}-lr-\" + str(\n",
        "      lr) + \"-train_loss-{loss:.4f}-val_loss-{val_loss:.4f}.hdf5\"\n",
        "\n",
        "  reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
        "\n",
        "  checkpoint = ModelCheckpoint(file_path,\n",
        "                              monitor=['loss'],\n",
        "                              verbose=1,\n",
        "                              save_best_only=False,\n",
        "                              save_weights_only=False,\n",
        "                              mode='min',\n",
        "                              period=ckpt_period)\n",
        "\n",
        "  tensorboard = TensorBoard(log_dir=\"/content/drive/My Drive/new-logs-3/{}\".format(time()), histogram_freq=0)\n",
        "\n",
        "  if n_epochs_to_train <= initial_epoch:\n",
        "      n_epochs_to_train += initial_epoch\n",
        "\n",
        "  model.fit_generator(generator=training_generator,\n",
        "                      validation_data=validation_generator,\n",
        "                      use_multiprocessing=True,\n",
        "                      epochs=n_epochs_to_train,\n",
        "                      initial_epoch=initial_epoch,\n",
        "                      callbacks=[checkpoint, tensorboard, reduce_lr_loss],\n",
        "                      workers=6)\n",
        "  K.clear_session()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTaemEPhw59J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sampling()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMvguz1W5GuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lab_image_record()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7BQV3245LuC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resnet_record()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1DjawS2DDuq",
        "colab_type": "code",
        "outputId": "8653c88d-3aed-4ce4-95df-c227a1b5cf64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls '/content/drive/My Drive/sampled' | wc -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgX1eODEiCg1",
        "colab_type": "code",
        "outputId": "c1acf720-4463-443b-fc66-444773c71c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls '/content/drive/My Drive/lab_records' | wc -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbLgx5uIqvA1",
        "colab_type": "code",
        "outputId": "cc12c462-67a0-404a-d740-daa7b77e04a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls '/content/drive/My Drive/resnet_csv_records' | wc -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ExSfaVy9aol",
        "colab_type": "code",
        "outputId": "f52206e0-8b64-47ed-eb8f-d653277bd3fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading from checkpoint: /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00123-lr-0.0001-train_loss-0.0020-val_loss-0.0017.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1335: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
            "  warnings.warn('`epsilon` argument is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 124/250\n",
            "547/547 [==============================] - 114s 209ms/step - loss: 0.0024 - acc: 0.6940 - val_loss: 0.0019 - val_acc: 0.7305\n",
            "Epoch 125/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0025 - acc: 0.7056 - val_loss: 0.0018 - val_acc: 0.6878\n",
            "Epoch 126/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0024 - acc: 0.7215 - val_loss: 0.0021 - val_acc: 0.6263\n",
            "\n",
            "Epoch 00126: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00126-lr-0.0001-train_loss-0.0024-val_loss-0.0021.hdf5\n",
            "Epoch 127/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0022 - acc: 0.7113 - val_loss: 0.0018 - val_acc: 0.6221\n",
            "Epoch 128/250\n",
            "547/547 [==============================] - 109s 199ms/step - loss: 0.0022 - acc: 0.7212 - val_loss: 0.0018 - val_acc: 0.7243\n",
            "Epoch 129/250\n",
            "547/547 [==============================] - 109s 199ms/step - loss: 0.0022 - acc: 0.7385 - val_loss: 0.0018 - val_acc: 0.6993\n",
            "\n",
            "Epoch 00129: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00129-lr-0.0001-train_loss-0.0022-val_loss-0.0018.hdf5\n",
            "Epoch 130/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0021 - acc: 0.7375 - val_loss: 0.0017 - val_acc: 0.7007\n",
            "Epoch 131/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0020 - acc: 0.7396 - val_loss: 0.0017 - val_acc: 0.7510\n",
            "Epoch 131/250Epoch 132/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0020 - acc: 0.7343 - val_loss: 0.0017 - val_acc: 0.7016\n",
            "\n",
            "Epoch 00132: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00132-lr-0.0001-train_loss-0.0020-val_loss-0.0017.hdf5\n",
            "Epoch 133/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0020 - acc: 0.7347 - val_loss: 0.0017 - val_acc: 0.7191\n",
            "Epoch 134/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0019 - acc: 0.7251 - val_loss: 0.0016 - val_acc: 0.7165\n",
            "Epoch 135/250\n",
            "547/547 [==============================] - 109s 198ms/step - loss: 0.0019 - acc: 0.7541 - val_loss: 0.0017 - val_acc: 0.7328\n",
            "\n",
            "Epoch 00135: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00135-lr-0.0001-train_loss-0.0019-val_loss-0.0017.hdf5\n",
            "Epoch 136/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0019 - acc: 0.7205 - val_loss: 0.0017 - val_acc: 0.6545\n",
            "Epoch 137/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0017 - acc: 0.7423 - val_loss: 0.0016 - val_acc: 0.7292\n",
            "Epoch 138/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0017 - acc: 0.7504 - val_loss: 0.0018 - val_acc: 0.6804\n",
            "\n",
            "Epoch 00138: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00138-lr-0.0001-train_loss-0.0017-val_loss-0.0018.hdf5\n",
            "Epoch 139/250\n",
            "547/547 [==============================] - 109s 199ms/step - loss: 0.0017 - acc: 0.7448 - val_loss: 0.0016 - val_acc: 0.6395\n",
            "Epoch 140/250\n",
            "547/547 [==============================] - 106s 194ms/step - loss: 0.0016 - acc: 0.7427 - val_loss: 0.0016 - val_acc: 0.6837\n",
            "Epoch 141/250\n",
            "546/547 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.7215\n",
            "547/547 [==============================] - 107s 195ms/step - loss: 0.0017 - acc: 0.7215 - val_loss: 0.0016 - val_acc: 0.7257\n",
            "\n",
            "Epoch 00141: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00141-lr-0.0001-train_loss-0.0017-val_loss-0.0016.hdf5\n",
            "\n",
            "Epoch 00141: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "Epoch 142/250\n",
            "547/547 [==============================] - 106s 194ms/step - loss: 0.0014 - acc: 0.7436 - val_loss: 0.0014 - val_acc: 0.7060\n",
            "Epoch 143/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0013 - acc: 0.7415 - val_loss: 0.0014 - val_acc: 0.7237\n",
            "Epoch 144/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0013 - acc: 0.7540 - val_loss: 0.0014 - val_acc: 0.7242\n",
            "\n",
            "Epoch 00144: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00144-lr-0.0001-train_loss-0.0013-val_loss-0.0014.hdf5\n",
            "Epoch 145/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0013 - acc: 0.7516 - val_loss: 0.0014 - val_acc: 0.7163\n",
            "Epoch 146/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0013 - acc: 0.7603 - val_loss: 0.0014 - val_acc: 0.7097\n",
            "Epoch 147/250\n",
            "\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0013 - acc: 0.7589 - val_loss: 0.0014 - val_acc: 0.7193\n",
            "\n",
            "Epoch 00147: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00147-lr-0.0001-train_loss-0.0013-val_loss-0.0014.hdf5\n",
            "Epoch 148/250\n",
            "546/547 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.7604\n",
            "Epoch 00147: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00147-lr-0.0001-train_loss-0.0013-val_loss-0.0014.hdf5\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0013 - acc: 0.7606 - val_loss: 0.0014 - val_acc: 0.7291\n",
            "Epoch 149/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7687 - val_loss: 0.0014 - val_acc: 0.7221\n",
            "\n",
            "Epoch 00149: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "Epoch 150/250\n",
            "547/547 [==============================] - 107s 197ms/step - loss: 0.0012 - acc: 0.7656 - val_loss: 0.0013 - val_acc: 0.7237\n",
            "\n",
            "Epoch 00150: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00150-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 151/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7691 - val_loss: 0.0013 - val_acc: 0.7228\n",
            "Epoch 152/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7680 - val_loss: 0.0013 - val_acc: 0.7274\n",
            "Epoch 153/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7678 - val_loss: 0.0013 - val_acc: 0.7243\n",
            "\n",
            "Epoch 00153: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00153-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 154/250\n",
            "546/547 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.7679\n",
            "Epoch 00153: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00153-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7679 - val_loss: 0.0013 - val_acc: 0.7239\n",
            "Epoch 155/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7665 - val_loss: 0.0013 - val_acc: 0.7260\n",
            "Epoch 156/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7679 - val_loss: 0.0013 - val_acc: 0.7274\n",
            "\n",
            "Epoch 00156: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00156-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "\n",
            "\n",
            "Epoch 00156: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "Epoch 157/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7693 - val_loss: 0.0013 - val_acc: 0.7271\n",
            "Epoch 158/250\n",
            "546/547 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.7686Epoch 158/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7687 - val_loss: 0.0013 - val_acc: 0.7269\n",
            "Epoch 159/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7682 - val_loss: 0.0013 - val_acc: 0.7263\n",
            "\n",
            "Epoch 00159: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00159-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 160/250\n",
            "546/547 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.7683\n",
            "Epoch 00159: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00159-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7264\n",
            "Epoch 161/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7680 - val_loss: 0.0013 - val_acc: 0.7264\n",
            "Epoch 162/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7680 - val_loss: 0.0013 - val_acc: 0.7267\n",
            "\n",
            "Epoch 00162: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00162-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 163/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7268\n",
            "\n",
            "Epoch 00163: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
            "Epoch 164/250\n",
            "\n",
            "547/547 [==============================] - 109s 199ms/step - loss: 0.0012 - acc: 0.7687 - val_loss: 0.0013 - val_acc: 0.7266\n",
            "Epoch 165/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7266\n",
            "\n",
            "Epoch 00165: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00165-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 166/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7267\n",
            "Epoch 167/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7270\n",
            "Epoch 168/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7686 - val_loss: 0.0013 - val_acc: 0.7270\n",
            "\n",
            "Epoch 00168: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00168-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 169/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7686 - val_loss: 0.0013 - val_acc: 0.7264\n",
            "Epoch 170/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7264\n",
            "\n",
            "Epoch 00170: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
            "Epoch 171/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7683 - val_loss: 0.0013 - val_acc: 0.7267\n",
            "\n",
            "Epoch 00171: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00171-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 172/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7683 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "Epoch 173/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7266\n",
            "Epoch 174/250\n",
            "547/547 [==============================] - 106s 194ms/step - loss: 0.0012 - acc: 0.7683 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "\n",
            "Epoch 00174: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00174-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 175/250\n",
            "547/547 [==============================] - 106s 193ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7268\n",
            "Epoch 176/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7683 - val_loss: 0.0013 - val_acc: 0.7267\n",
            "Epoch 177/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7263\n",
            "\n",
            "Epoch 00177: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00177-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "\n",
            "Epoch 00177: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
            "Epoch 178/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7686 - val_loss: 0.0013 - val_acc: 0.7263\n",
            "Epoch 179/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7687 - val_loss: 0.0013 - val_acc: 0.7267\n",
            "Epoch 180/250\n",
            "Epoch 179/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7683 - val_loss: 0.0013 - val_acc: 0.7266\n",
            "\n",
            "Epoch 00180: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00180-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 181/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7267\n",
            "Epoch 182/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7683 - val_loss: 0.0013 - val_acc: 0.7266\n",
            "Epoch 183/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7267\n",
            "\n",
            "Epoch 00183: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00183-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 184/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7268\n",
            "\n",
            "Epoch 00184: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
            "Epoch 185/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7264\n",
            "Epoch 186/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7682 - val_loss: 0.0013 - val_acc: 0.7267\n",
            "\n",
            "Epoch 00186: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00186-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 187/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7264\n",
            "Epoch 188/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "Epoch 189/250\n",
            "Epoch 188/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7683 - val_loss: 0.0013 - val_acc: 0.7260\n",
            "\n",
            "Epoch 00189: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00189-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 190/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7683 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "Epoch 191/250\n",
            "546/547 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.7686Epoch 191/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7686 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "\n",
            "Epoch 00191: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
            "Epoch 192/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7267\n",
            "\n",
            "Epoch 00192: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00192-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 193/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "Epoch 194/250\n",
            "\n",
            "Epoch 00192: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00192-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7270\n",
            "Epoch 195/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7687 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "\n",
            "Epoch 00195: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00195-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 195/250\n",
            "Epoch 196/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7263\n",
            "Epoch 197/250\n",
            "547/547 [==============================] - 109s 199ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7262\n",
            "Epoch 198/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7262\n",
            "\n",
            "Epoch 00198: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00198-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "\n",
            "Epoch 00198: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
            "Epoch 199/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7270\n",
            "Epoch 200/250\n",
            "547/547 [==============================] - 109s 199ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7267\n",
            "Epoch 201/250\n",
            "547/547 [==============================] - 109s 199ms/step - loss: 0.0012 - acc: 0.7682 - val_loss: 0.0013 - val_acc: 0.7270\n",
            "\n",
            "Epoch 00201: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00201-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 202/250\n",
            "546/547 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.7685\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7270\n",
            "Epoch 203/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "Epoch 204/250\n",
            "Epoch 203/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7681 - val_loss: 0.0013 - val_acc: 0.7267\n",
            "\n",
            "Epoch 00204: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00204-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 205/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7686 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "\n",
            "Epoch 00205: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
            "Epoch 206/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7683 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "Epoch 207/250\n",
            "\n",
            "547/547 [==============================] - 107s 195ms/step - loss: 0.0012 - acc: 0.7683 - val_loss: 0.0013 - val_acc: 0.7267\n",
            "\n",
            "Epoch 00207: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00207-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 207/250\n",
            "Epoch 208/250\n",
            "547/547 [==============================] - 106s 194ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7266\n",
            "Epoch 209/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7264\n",
            "Epoch 210/250\n",
            "546/547 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.7682Epoch 210/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7683 - val_loss: 0.0013 - val_acc: 0.7269\n",
            "\n",
            "Epoch 00210: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00210-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 211/250\n",
            "547/547 [==============================] - 106s 194ms/step - loss: 0.0012 - acc: 0.7686 - val_loss: 0.0013 - val_acc: 0.7266\n",
            "Epoch 212/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7683 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "\n",
            "Epoch 00212: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
            "Epoch 213/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7687 - val_loss: 0.0013 - val_acc: 0.7268\n",
            "\n",
            "Epoch 00213: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00213-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 214/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "Epoch 215/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7267\n",
            "Epoch 216/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7682 - val_loss: 0.0013 - val_acc: 0.7269\n",
            "\n",
            "Epoch 00216: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00216-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 217/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "Epoch 218/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "Epoch 219/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7264\n",
            "\n",
            "Epoch 00219: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00219-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "\n",
            "Epoch 00219: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n",
            "Epoch 220/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7685 - val_loss: 0.0013 - val_acc: 0.7264\n",
            "Epoch 221/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7686 - val_loss: 0.0013 - val_acc: 0.7263\n",
            "Epoch 222/250\n",
            "Epoch 221/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7683 - val_loss: 0.0013 - val_acc: 0.7266\n",
            "\n",
            "Epoch 00222: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00222-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 223/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7268\n",
            "Epoch 224/250\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7686 - val_loss: 0.0013 - val_acc: 0.7261\n",
            "Epoch 225/250\n",
            "547/547 [==============================] - 109s 200ms/step - loss: 0.0012 - acc: 0.7683 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "\n",
            "Epoch 00225: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00225-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 226/250\n",
            "547/547 [==============================] - 107s 196ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7265\n",
            "\n",
            "Epoch 00226: ReduceLROnPlateau reducing learning rate to 9.999998845134856e-18.\n",
            "Epoch 227/250\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "547/547 [==============================] - 108s 198ms/step - loss: 0.0012 - acc: 0.7684 - val_loss: 0.0013 - val_acc: 0.7261\n",
            "Epoch 228/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7686 - val_loss: 0.0013 - val_acc: 0.7267\n",
            "\n",
            "Epoch 00228: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00228-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 228/250\n",
            "Epoch 229/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7682 - val_loss: 0.0013 - val_acc: 0.7266\n",
            "Epoch 230/250\n",
            "547/547 [==============================] - 109s 199ms/step - loss: 0.0012 - acc: 0.7687 - val_loss: 0.0013 - val_acc: 0.7263\n",
            "Epoch 231/250\n",
            "546/547 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.7686Epoch 231/250\n",
            "547/547 [==============================] - 108s 197ms/step - loss: 0.0012 - acc: 0.7687 - val_loss: 0.0013 - val_acc: 0.7270\n",
            "\n",
            "Epoch 00231: saving model to /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00231-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Epoch 232/250\n",
            "349/547 [==================>...........] - ETA: 28s - loss: 0.0012 - acc: 0.7687Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "827P8MpB1U7c",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAbHs6hMoJNJ",
        "colab_type": "code",
        "outputId": "3d362b74-b206-42ec-8c08-5ebd60dfbeae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "# inference()\n",
        "process_test_file(\"squirrel.mp4\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running resnet model\n",
            "Combining L layer and resnet out\n",
            "loading from checkpoint: /content/drive/My Drive/new-checkpoints-3/flowchroma-epoch-00249-lr-0.0001-train_loss-0.0012-val_loss-0.0013.hdf5\n",
            "Flowchroma model predictions calculated\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}